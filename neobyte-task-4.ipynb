{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30886,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# NeoByte Task - 4\n\n\n\n\n* For this task, I chose to go ahead with developing a **Classification Model** (Titanic Survival Prediction).\n* The dataset that I chose was: **\"Titanic: Machine Learning from Disaster\"**\n\n\n","metadata":{}},{"cell_type":"markdown","source":"# Loading and checking Dataset","metadata":{}},{"cell_type":"code","source":"import pandas as pd\n\n# Load the datasets\ntrain_df = pd.read_csv(\"/kaggle/input/titanic/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/titanic/test.csv\")\n\n# Display first few rows\nprint(\"Train Data:\")\nprint(train_df.head())\n\nprint(\"\\nTest Data:\")\nprint(test_df.head())\n\n# Check dataset info\nprint(\"\\nTrain Data Info:\")\nprint(train_df.info())\n\nprint(\"\\nMissing Values in Train Data:\")\nprint(train_df.isnull().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:19:11.938392Z","iopub.execute_input":"2025-02-13T14:19:11.938732Z","iopub.status.idle":"2025-02-13T14:19:12.404092Z","shell.execute_reply.started":"2025-02-13T14:19:11.938704Z","shell.execute_reply":"2025-02-13T14:19:12.403124Z"}},"outputs":[{"name":"stdout","text":"Train Data:\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name     Sex   Age  SibSp  \\\n0                            Braund, Mr. Owen Harris    male  22.0      1   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n2                             Heikkinen, Miss. Laina  female  26.0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n4                           Allen, Mr. William Henry    male  35.0      0   \n\n   Parch            Ticket     Fare Cabin Embarked  \n0      0         A/5 21171   7.2500   NaN        S  \n1      0          PC 17599  71.2833   C85        C  \n2      0  STON/O2. 3101282   7.9250   NaN        S  \n3      0            113803  53.1000  C123        S  \n4      0            373450   8.0500   NaN        S  \n\nTest Data:\n   PassengerId  Pclass                                          Name     Sex  \\\n0          892       3                              Kelly, Mr. James    male   \n1          893       3              Wilkes, Mrs. James (Ellen Needs)  female   \n2          894       2                     Myles, Mr. Thomas Francis    male   \n3          895       3                              Wirz, Mr. Albert    male   \n4          896       3  Hirvonen, Mrs. Alexander (Helga E Lindqvist)  female   \n\n    Age  SibSp  Parch   Ticket     Fare Cabin Embarked  \n0  34.5      0      0   330911   7.8292   NaN        Q  \n1  47.0      1      0   363272   7.0000   NaN        S  \n2  62.0      0      0   240276   9.6875   NaN        Q  \n3  27.0      0      0   315154   8.6625   NaN        S  \n4  22.0      1      1  3101298  12.2875   NaN        S  \n\nTrain Data Info:\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 891 entries, 0 to 890\nData columns (total 12 columns):\n #   Column       Non-Null Count  Dtype  \n---  ------       --------------  -----  \n 0   PassengerId  891 non-null    int64  \n 1   Survived     891 non-null    int64  \n 2   Pclass       891 non-null    int64  \n 3   Name         891 non-null    object \n 4   Sex          891 non-null    object \n 5   Age          714 non-null    float64\n 6   SibSp        891 non-null    int64  \n 7   Parch        891 non-null    int64  \n 8   Ticket       891 non-null    object \n 9   Fare         891 non-null    float64\n 10  Cabin        204 non-null    object \n 11  Embarked     889 non-null    object \ndtypes: float64(2), int64(5), object(5)\nmemory usage: 83.7+ KB\nNone\n\nMissing Values in Train Data:\nPassengerId      0\nSurvived         0\nPclass           0\nName             0\nSex              0\nAge            177\nSibSp            0\nParch            0\nTicket           0\nFare             0\nCabin          687\nEmbarked         2\ndtype: int64\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"# Data Preprocessing","metadata":{}},{"cell_type":"markdown","source":"**Handling Missing Values**","metadata":{}},{"cell_type":"code","source":"# Handling missing values\n\n# Filling missing Age values with the median\ntrain_df[\"Age\"].fillna(train_df[\"Age\"].median(), inplace=True)\ntest_df[\"Age\"].fillna(test_df[\"Age\"].median(), inplace=True)\n\n# Droping Cabin column (too many missing values)\n#train_df.drop(columns=[\"Cabin\"], inplace=True)\n#test_df.drop(columns=[\"Cabin\"], inplace=True)\n\n# Filling missing cabin values with 'Unknown' and extract only the first letter (deck)\ntrain_df[\"Cabin\"] = train_df[\"Cabin\"].fillna(\"Unknown\").apply(lambda x: x[0])\ntest_df[\"Cabin\"] = test_df[\"Cabin\"].fillna(\"Unknown\").apply(lambda x: x[0])\n\n# Grouping less frequent decks as \"Other\"\ndeck_counts = train_df[\"Cabin\"].value_counts()\nrare_decks = deck_counts[deck_counts < 10].index  # Adjust threshold as needed\ntrain_df[\"Cabin\"] = train_df[\"Cabin\"].apply(lambda x: \"Other\" if x in rare_decks else x)\ntest_df[\"Cabin\"] = test_df[\"Cabin\"].apply(lambda x: \"Other\" if x in rare_decks else x)\n\n# Filling missing Embarked values with most frequent value (mode)\ntrain_df[\"Embarked\"].fillna(train_df[\"Embarked\"].mode()[0], inplace=True)\ntest_df[\"Embarked\"].fillna(train_df[\"Embarked\"].mode()[0], inplace=True)\n\n# Checking again for missing values\nprint(\"Missing Values after Cleaning:\")\nprint(train_df.isnull().sum())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:19:28.884130Z","iopub.execute_input":"2025-02-13T14:19:28.884488Z","iopub.status.idle":"2025-02-13T14:19:28.905498Z","shell.execute_reply.started":"2025-02-13T14:19:28.884458Z","shell.execute_reply":"2025-02-13T14:19:28.904210Z"}},"outputs":[{"name":"stdout","text":"Missing Values after Cleaning:\nPassengerId    0\nSurvived       0\nPclass         0\nName           0\nSex            0\nAge            0\nSibSp          0\nParch          0\nTicket         0\nFare           0\nCabin          0\nEmbarked       0\ndtype: int64\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-2-80ab6fe9a8d2>:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  train_df[\"Age\"].fillna(train_df[\"Age\"].median(), inplace=True)\n<ipython-input-2-80ab6fe9a8d2>:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  test_df[\"Age\"].fillna(test_df[\"Age\"].median(), inplace=True)\n<ipython-input-2-80ab6fe9a8d2>:22: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  train_df[\"Embarked\"].fillna(train_df[\"Embarked\"].mode()[0], inplace=True)\n<ipython-input-2-80ab6fe9a8d2>:23: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  test_df[\"Embarked\"].fillna(train_df[\"Embarked\"].mode()[0], inplace=True)\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"**Encoding Categorical Features**\n\nMachine Learning models work with numerical data, so we need to convert categorical values into numbers.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\n# Converting \"Sex\" column to numerical (Male = 0, Female = 1)\ntrain_df[\"Sex\"] = train_df[\"Sex\"].map({\"male\": 0, \"female\": 1})\ntest_df[\"Sex\"] = test_df[\"Sex\"].map({\"male\": 0, \"female\": 1})\n\n# One-Hot Encode \"Cabin\"\ntrain_df = pd.get_dummies(train_df, columns=[\"Cabin\"], drop_first=True)\ntest_df = pd.get_dummies(test_df, columns=[\"Cabin\"], drop_first=True)\n\n# One-Hot Encoding for \"Embarked\" column\ntrain_df = pd.get_dummies(train_df, columns=[\"Embarked\"], drop_first=True)\ntest_df = pd.get_dummies(test_df, columns=[\"Embarked\"], drop_first=True)\n\n# Ensuring test set has the same feature columns as train set\ntrain_columns = train_df.columns\ntest_df = test_df.reindex(columns=train_columns, fill_value=0)\n\n# Checking for any remaining missing values\nprint(\"Missing Values in Train Set:\\n\", train_df.isnull().sum().sum())\nprint(\"Missing Values in Test Set:\\n\", test_df.isnull().sum().sum())\n\n# Displaying first few rows after encoding\nprint(train_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:19:48.460914Z","iopub.execute_input":"2025-02-13T14:19:48.461268Z","iopub.status.idle":"2025-02-13T14:19:49.049736Z","shell.execute_reply.started":"2025-02-13T14:19:48.461239Z","shell.execute_reply":"2025-02-13T14:19:49.048648Z"}},"outputs":[{"name":"stdout","text":"Missing Values in Train Set:\n 0\nMissing Values in Test Set:\n 1\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name  Sex   Age  SibSp  Parch  \\\n0                            Braund, Mr. Owen Harris    0  22.0      1      0   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...    1  38.0      1      0   \n2                             Heikkinen, Miss. Laina    1  26.0      0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    1  35.0      1      0   \n4                           Allen, Mr. William Henry    0  35.0      0      0   \n\n             Ticket     Fare  Cabin_B  Cabin_C  Cabin_D  Cabin_E  Cabin_F  \\\n0         A/5 21171   7.2500    False    False    False    False    False   \n1          PC 17599  71.2833    False     True    False    False    False   \n2  STON/O2. 3101282   7.9250    False    False    False    False    False   \n3            113803  53.1000    False     True    False    False    False   \n4            373450   8.0500    False    False    False    False    False   \n\n   Cabin_Other  Cabin_U  Embarked_Q  Embarked_S  \n0        False     True       False        True  \n1        False    False       False       False  \n2        False     True       False        True  \n3        False    False       False        True  \n4        False     True       False        True  \n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"**Fixing Missing Values in Test Set**","metadata":{}},{"cell_type":"code","source":"print(test_df.isnull().sum()[test_df.isnull().sum() > 0]) #checks which column has NaN value in test set","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:19:58.320456Z","iopub.execute_input":"2025-02-13T14:19:58.320989Z","iopub.status.idle":"2025-02-13T14:19:58.330262Z","shell.execute_reply.started":"2025-02-13T14:19:58.320956Z","shell.execute_reply":"2025-02-13T14:19:58.329209Z"}},"outputs":[{"name":"stdout","text":"Fare    1\ndtype: int64\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"test_df[\"Fare\"].fillna(test_df[\"Fare\"].median(), inplace=True) #Fixing the missing value\nprint(\"Missing Values in Test Set:\\n\", test_df.isnull().sum().sum()) #again checking for missing val4ues. ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:20:10.812966Z","iopub.execute_input":"2025-02-13T14:20:10.813417Z","iopub.status.idle":"2025-02-13T14:20:10.823449Z","shell.execute_reply.started":"2025-02-13T14:20:10.813380Z","shell.execute_reply":"2025-02-13T14:20:10.822115Z"}},"outputs":[{"name":"stdout","text":"Missing Values in Test Set:\n 0\n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-6-259eb664adda>:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\nThe behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n\nFor example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n\n\n  test_df[\"Fare\"].fillna(test_df[\"Fare\"].median(), inplace=True) #Fixing the missing value\n","output_type":"stream"}],"execution_count":6},{"cell_type":"markdown","source":"**Feature Selection & Scaling**","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler\n\n# Selecting features for training\nfeatures = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked_Q\", \"Embarked_S\"]\n\n# Adding all Cabin-related features dynamically\ncabin_features = [col for col in train_df.columns if col.startswith(\"Cabin_\")]\nfeatures.extend(cabin_features)  \n\nX_train = train_df[features]\ny_train = train_df[\"Survived\"]\nX_test = test_df[features]\n\n# Scaling \"Age\" and \"Fare\" using StandardScaler\nscaler = StandardScaler()\nX_train[[\"Age\", \"Fare\"]] = scaler.fit_transform(X_train[[\"Age\", \"Fare\"]])\nX_test[[\"Age\", \"Fare\"]] = scaler.transform(X_test[[\"Age\", \"Fare\"]])  # Apply same scaling\n\n# Display first few rows after scaling\nprint(X_train.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:20:37.581458Z","iopub.execute_input":"2025-02-13T14:20:37.581795Z","iopub.status.idle":"2025-02-13T14:20:37.603031Z","shell.execute_reply.started":"2025-02-13T14:20:37.581767Z","shell.execute_reply":"2025-02-13T14:20:37.601666Z"}},"outputs":[{"name":"stdout","text":"   Pclass  Sex       Age  SibSp  Parch      Fare  Embarked_Q  Embarked_S  \\\n0       3    0 -0.565736      1      0 -0.502445       False        True   \n1       1    1  0.663861      1      0  0.786845       False       False   \n2       3    1 -0.258337      0      0 -0.488854       False        True   \n3       1    1  0.433312      1      0  0.420730       False        True   \n4       3    0  0.433312      0      0 -0.486337       False        True   \n\n   Cabin_B  Cabin_C  Cabin_D  Cabin_E  Cabin_F  Cabin_Other  Cabin_U  \n0    False    False    False    False    False        False     True  \n1    False     True    False    False    False        False    False  \n2    False    False    False    False    False        False     True  \n3    False     True    False    False    False        False    False  \n4    False    False    False    False    False        False     True  \n","output_type":"stream"},{"name":"stderr","text":"<ipython-input-8-f5943261ef30>:16: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X_train[[\"Age\", \"Fare\"]] = scaler.fit_transform(X_train[[\"Age\", \"Fare\"]])\n<ipython-input-8-f5943261ef30>:17: SettingWithCopyWarning: \nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n  X_test[[\"Age\", \"Fare\"]] = scaler.transform(X_test[[\"Age\", \"Fare\"]])  # Apply same scaling\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"%who #checks all saved variables\n\nprint(train_df.dtypes)  # Check data types of all columns\nprint(train_df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:20:49.488804Z","iopub.execute_input":"2025-02-13T14:20:49.489222Z","iopub.status.idle":"2025-02-13T14:20:49.503140Z","shell.execute_reply.started":"2025-02-13T14:20:49.489193Z","shell.execute_reply":"2025-02-13T14:20:49.501916Z"}},"outputs":[{"name":"stdout","text":"No variables match your requested type.\nPassengerId      int64\nSurvived         int64\nPclass           int64\nName            object\nSex              int64\nAge            float64\nSibSp            int64\nParch            int64\nTicket          object\nFare           float64\nCabin_B           bool\nCabin_C           bool\nCabin_D           bool\nCabin_E           bool\nCabin_F           bool\nCabin_Other       bool\nCabin_U           bool\nEmbarked_Q        bool\nEmbarked_S        bool\ndtype: object\n   PassengerId  Survived  Pclass  \\\n0            1         0       3   \n1            2         1       1   \n2            3         1       3   \n3            4         1       1   \n4            5         0       3   \n\n                                                Name  Sex   Age  SibSp  Parch  \\\n0                            Braund, Mr. Owen Harris    0  22.0      1      0   \n1  Cumings, Mrs. John Bradley (Florence Briggs Th...    1  38.0      1      0   \n2                             Heikkinen, Miss. Laina    1  26.0      0      0   \n3       Futrelle, Mrs. Jacques Heath (Lily May Peel)    1  35.0      1      0   \n4                           Allen, Mr. William Henry    0  35.0      0      0   \n\n             Ticket     Fare  Cabin_B  Cabin_C  Cabin_D  Cabin_E  Cabin_F  \\\n0         A/5 21171   7.2500    False    False    False    False    False   \n1          PC 17599  71.2833    False     True    False    False    False   \n2  STON/O2. 3101282   7.9250    False    False    False    False    False   \n3            113803  53.1000    False     True    False    False    False   \n4            373450   8.0500    False    False    False    False    False   \n\n   Cabin_Other  Cabin_U  Embarked_Q  Embarked_S  \n0        False     True       False        True  \n1        False    False       False       False  \n2        False     True       False        True  \n3        False    False       False        True  \n4        False     True       False        True  \n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"# Model Training","metadata":{}},{"cell_type":"markdown","source":"**Using Logistic Regression**","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, classification_report\n\n# Dropping \"Name\" and \"Ticket\" since they are non-numeric\ntrain_df = train_df.drop([\"Name\", \"Ticket\"], axis=1)\ntest_df = test_df.drop([\"Name\", \"Ticket\"], axis=1)\n\n# Define Features (X) & Target (y)\nX = train_df.drop(\"Survived\", axis=1)  # Features\ny = train_df[\"Survived\"]  # Target variable\n\n# Split the Data (80% train, 20% test)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n#  Train Logistic Regression Model\nmodel = LogisticRegression()\nmodel.fit(X_train, y_train)\n\n#  Predict on Test Set\ny_pred = model.predict(X_test)\n\n#  Evaluate the Model\naccuracy = accuracy_score(y_test, y_pred)\nprint(f\"Model Accuracy: {accuracy:.4f}\")\nprint(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:21:00.668839Z","iopub.execute_input":"2025-02-13T14:21:00.669209Z","iopub.status.idle":"2025-02-13T14:21:00.980705Z","shell.execute_reply.started":"2025-02-13T14:21:00.669180Z","shell.execute_reply":"2025-02-13T14:21:00.979639Z"}},"outputs":[{"name":"stdout","text":"Model Accuracy: 0.8045\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.82      0.85      0.84       105\n           1       0.77      0.74      0.76        74\n\n    accuracy                           0.80       179\n   macro avg       0.80      0.80      0.80       179\nweighted avg       0.80      0.80      0.80       179\n\n","output_type":"stream"},{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\nSTOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n\nIncrease the number of iterations (max_iter) or scale the data as shown in:\n    https://scikit-learn.org/stable/modules/preprocessing.html\nPlease also refer to the documentation for alternative solver options:\n    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n  n_iter_i = _check_optimize_result(\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"**Using Random Forest Classifier**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report\nimport pandas as pd\n\n# Define Features and Target\nX = train_df.drop(columns=[\"Survived\", \"PassengerId\"])  # Drop target and ID\ny = train_df[\"Survived\"]\n\n# Train-Validation Split\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\n# Standardizing \"Age\" and \"Fare\"\nscaler = StandardScaler()\nX_train[[\"Age\", \"Fare\"]] = scaler.fit_transform(X_train[[\"Age\", \"Fare\"]])\nX_val[[\"Age\", \"Fare\"]] = scaler.transform(X_val[[\"Age\", \"Fare\"]])\n\n# Initialize & Train Random Forest Classifier\nrf_model = RandomForestClassifier(\n    n_estimators=150,  # Increased trees for better learning\n    max_depth=10,      # Limiting depth to prevent overfitting\n    min_samples_split=5,\n    min_samples_leaf=2,\n    random_state=42\n)\nrf_model.fit(X_train, y_train)\n\n# Predictions\ny_pred = rf_model.predict(X_val)\n\n# Evaluation Metrics\nprint(\"Accuracy:\", accuracy_score(y_val, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\n\n# Feature Importance\nimportances = rf_model.feature_importances_\nfeature_names = X.columns\nfeature_importance_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances}).sort_values(by=\"Importance\", ascending=False)\n\n# Display Top 10 Features\nprint(\"\\nTop 10 Important Features:\\n\", feature_importance_df.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:21:26.556183Z","iopub.execute_input":"2025-02-13T14:21:26.556614Z","iopub.status.idle":"2025-02-13T14:21:27.185890Z","shell.execute_reply.started":"2025-02-13T14:21:26.556584Z","shell.execute_reply":"2025-02-13T14:21:27.184758Z"}},"outputs":[{"name":"stdout","text":"Accuracy: 0.7988826815642458\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.80      0.89      0.84       110\n           1       0.79      0.65      0.71        69\n\n    accuracy                           0.80       179\n   macro avg       0.80      0.77      0.78       179\nweighted avg       0.80      0.80      0.79       179\n\n\nTop 10 Important Features:\n        Feature  Importance\n1          Sex    0.339677\n5         Fare    0.190863\n2          Age    0.148483\n0       Pclass    0.093968\n12     Cabin_U    0.064622\n3        SibSp    0.043961\n4        Parch    0.037046\n14  Embarked_S    0.026563\n9      Cabin_E    0.011781\n8      Cabin_D    0.010924\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"**Tried to fine-tune random forest classifier**","metadata":{}},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score, classification_report\nimport pandas as pd\nimport numpy as np\n\n# Define Features and Target\nX = train_df.drop(columns=[\"Survived\", \"PassengerId\"])  # Drop target and ID\ny = train_df[\"Survived\"]\n\n# Train-Validation Split (70-30 for better generalization)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Standardizing \"Age\" and \"Fare\"\nscaler = StandardScaler()\nX_train[[\"Age\", \"Fare\"]] = scaler.fit_transform(X_train[[\"Age\", \"Fare\"]])\nX_val[[\"Age\", \"Fare\"]] = scaler.transform(X_val[[\"Age\", \"Fare\"]])\n\n# Expanded Hyperparameter Grid for Random Forest\nparam_grid = {\n    \"n_estimators\": [100, 300, 500, 700],         # More trees\n    \"max_depth\": [10, 20, 30, None],              # Allow deeper trees\n    \"min_samples_split\": [2, 5, 10, 15],          # Tuning split size\n    \"min_samples_leaf\": [1, 2, 4, 6],             # Leaf size variations\n    \"max_features\": [\"sqrt\", \"log2\", None],       # Feature selection for splits\n    \"bootstrap\": [True, False]                    # Bootstrapping on/off\n}\n\n# Initialize Random Forest Model\nrf_model = RandomForestClassifier(random_state=42)\n\n# Randomized Search for Best Hyperparameters\nrf_search = RandomizedSearchCV(\n    estimator=rf_model,\n    param_distributions=param_grid,\n    n_iter=30,  # Increased search space\n    cv=5,       # 5-Fold Cross-Validation\n    verbose=2,\n    n_jobs=-1,\n    random_state=42\n)\n\n# Fit RandomizedSearchCV\nrf_search.fit(X_train, y_train)\n\n# Best Model from Search\nbest_rf_model = rf_search.best_estimator_\n\n# Predictions\ny_pred = best_rf_model.predict(X_val)\n\n# Evaluation Metrics\nprint(\"\\nBest Parameters:\", rf_search.best_params_)\nprint(\"\\nAccuracy:\", accuracy_score(y_val, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\n\n# Feature Importance\nimportances = best_rf_model.feature_importances_\nfeature_names = X.columns\nfeature_importance_df = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances}).sort_values(by=\"Importance\", ascending=False)\n\n# Display Top 10 Features\nprint(\"\\nTop 10 Important Features:\\n\", feature_importance_df.head(10))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:21:42.311800Z","iopub.execute_input":"2025-02-13T14:21:42.312208Z","iopub.status.idle":"2025-02-13T14:22:25.195902Z","shell.execute_reply.started":"2025-02-13T14:21:42.312177Z","shell.execute_reply":"2025-02-13T14:22:25.194564Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 30 candidates, totalling 150 fits\n\nBest Parameters: {'n_estimators': 500, 'min_samples_split': 15, 'min_samples_leaf': 2, 'max_features': 'sqrt', 'max_depth': 10, 'bootstrap': True}\n\nAccuracy: 0.7947761194029851\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.81      0.88      0.84       165\n           1       0.77      0.66      0.71       103\n\n    accuracy                           0.79       268\n   macro avg       0.79      0.77      0.78       268\nweighted avg       0.79      0.79      0.79       268\n\n\nTop 10 Important Features:\n        Feature  Importance\n1          Sex    0.388453\n5         Fare    0.160095\n2          Age    0.124298\n0       Pclass    0.092662\n12     Cabin_U    0.080392\n4        Parch    0.039977\n3        SibSp    0.035025\n14  Embarked_S    0.022977\n8      Cabin_D    0.012412\n9      Cabin_E    0.010837\n","output_type":"stream"}],"execution_count":12},{"cell_type":"markdown","source":"**Using XGBoost**","metadata":{}},{"cell_type":"code","source":"import xgboost as xgb\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.preprocessing import StandardScaler\n\n# Define Features and Target\nX = train_df.drop(columns=[\"Survived\", \"PassengerId\"])  # Drop target and ID\ny = train_df[\"Survived\"]\n\n# Train-Validation Split (70-30 for better generalization)\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n\n# Standardizing \"Age\" and \"Fare\"\nscaler = StandardScaler()\nX_train[[\"Age\", \"Fare\"]] = scaler.fit_transform(X_train[[\"Age\", \"Fare\"]])\nX_val[[\"Age\", \"Fare\"]] = scaler.transform(X_val[[\"Age\", \"Fare\"]])\n\n# Define XGBoost Classifier\nxgb_model = xgb.XGBClassifier(\n    objective=\"binary:logistic\",  # Binary classification\n    eval_metric=\"logloss\",\n    use_label_encoder=False,\n    random_state=42\n)\n\n# Hyperparameter Grid\nparam_grid = {\n    \"n_estimators\": [100, 300, 500, 700],   # More boosting rounds\n    \"learning_rate\": [0.01, 0.05, 0.1, 0.2],  # Step size\n    \"max_depth\": [3, 5, 7, 10],              # Tree depth\n    \"min_child_weight\": [1, 3, 5],           # Minimum child weight\n    \"gamma\": [0, 0.1, 0.2],                  # Minimum loss reduction\n    \"subsample\": [0.7, 0.8, 0.9],            # % of data used for training\n    \"colsample_bytree\": [0.7, 0.8, 1.0],     # Features used per tree\n    \"lambda\": [0, 0.1, 1.0],                 # L2 regularization\n    \"alpha\": [0, 0.1, 1.0]                   # L1 regularization\n}\n\n# Randomized Search for Best Hyperparameters\nxgb_search = RandomizedSearchCV(\n    estimator=xgb_model,\n    param_distributions=param_grid,\n    n_iter=30,  # Increased search space\n    cv=5,       # 5-Fold Cross-Validation\n    verbose=2,\n    n_jobs=-1,\n    random_state=42\n)\n\n# Train XGBoost\nxgb_search.fit(X_train, y_train)\n\n# Best Model\nbest_xgb_model = xgb_search.best_estimator_\n\n# Predictions\ny_pred = best_xgb_model.predict(X_val)\n\n# Evaluation Metrics\nprint(\"\\nBest Parameters:\", xgb_search.best_params_)\nprint(\"\\nAccuracy:\", accuracy_score(y_val, y_pred))\nprint(\"\\nClassification Report:\\n\", classification_report(y_val, y_pred))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:22:38.573223Z","iopub.execute_input":"2025-02-13T14:22:38.573629Z","iopub.status.idle":"2025-02-13T14:22:47.620583Z","shell.execute_reply.started":"2025-02-13T14:22:38.573595Z","shell.execute_reply":"2025-02-13T14:22:47.619151Z"}},"outputs":[{"name":"stdout","text":"Fitting 5 folds for each of 30 candidates, totalling 150 fits\n\nBest Parameters: {'subsample': 0.7, 'n_estimators': 100, 'min_child_weight': 1, 'max_depth': 3, 'learning_rate': 0.1, 'lambda': 0, 'gamma': 0.1, 'colsample_bytree': 0.7, 'alpha': 0.1}\n\nAccuracy: 0.8171641791044776\n\nClassification Report:\n               precision    recall  f1-score   support\n\n           0       0.82      0.90      0.86       165\n           1       0.81      0.68      0.74       103\n\n    accuracy                           0.82       268\n   macro avg       0.82      0.79      0.80       268\nweighted avg       0.82      0.82      0.81       268\n\n","output_type":"stream"}],"execution_count":13},{"cell_type":"markdown","source":"# Saving the trained model with model versioning","metadata":{}},{"cell_type":"code","source":"import os\nimport pickle\nimport re\n\n# Find existing model versions in the directory\nexisting_models = [f for f in os.listdir() if re.match(r\"xgb_model_v\\d+\\.pkl\", f)]\n\nif existing_models:\n    # Extract version numbers and find the latest\n    latest_version = max([int(re.search(r\"v(\\d+)\", f).group(1)) for f in existing_models])\n    new_version = latest_version + 1\nelse:\n    new_version = 1  # First version\n\n# Define filename with versioning\nfilename = f\"xgb_model_v{new_version}.pkl\"\n\n# Save the model with the new version\nwith open(filename, \"wb\") as file:\n    pickle.dump(best_xgb_model, file)\n\nprint(f\"Model saved successfully as '{filename}'\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-13T14:29:31.038511Z","iopub.execute_input":"2025-02-13T14:29:31.038874Z","iopub.status.idle":"2025-02-13T14:29:31.050172Z","shell.execute_reply.started":"2025-02-13T14:29:31.038847Z","shell.execute_reply":"2025-02-13T14:29:31.049133Z"}},"outputs":[{"name":"stdout","text":"Model saved successfully as 'xgb_model_v1.pkl'\n","output_type":"stream"}],"execution_count":16}]}